{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:78px\">Final Project IRWA (2024-2025)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:48px\">Part 2: Indexing and Evaluation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Third-party imports\n",
    "\n",
    "\n",
    "# Local application imports\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in locals() else os.getcwd()\n",
    "project_root = os.path.join(current_dir, '..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "import irwa.loading as ild \n",
    "import irwa.preprocessing as ipp\n",
    "import irwa.indexing as ind\n",
    "import irwa.ranking as irk\n",
    "\n",
    "# The following lines allow for autoreload of modules. They allow changes in modules without the need to reload the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 117407 tweets\n",
      "Loaded 48429 documents with their corresponding tokenized tweet content\n"
     ]
    }
   ],
   "source": [
    "# Loading and preprocessing\n",
    "file_path = '../data/farmers-protest-tweets.json'\n",
    "tweets = ild.load_tweets_from_json(file_path)\n",
    "print(f\"Loaded {len(tweets)} tweets\")\n",
    "tweet_document_ids_map_df = \"../data/tweet_document_ids_map.csv\"\n",
    "token_tweets = ipp.create_tokenized_dictionary(tweets, tweet_document_ids_map_df)\n",
    "print(f\"Loaded {len(token_tweets)} documents with their corresponding tokenized tweet content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverted Index construction\n",
    "inverted_index = ind.create_inverted_index(token_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of test queries\n",
    "query1 = [\"indian\", \"protest\"]       # Example given in handout\n",
    "query2 = [\"support\", \"farmers\"]      # Example given in handout\n",
    "query3 = [\"house\", \"india\"]\n",
    "query4 = [\"government\", \"corrupt\"]\n",
    "query5 = [\"president\", \"india\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Results:\n",
      "Document doc_13095: 13.035678814641825\n",
      "Document doc_445: 12.575039678288194\n",
      "Document doc_5374: 11.534076978962176\n",
      "Document doc_9022: 11.534076978962176\n",
      "Document doc_37090: 11.073437842608545\n"
     ]
    }
   ],
   "source": [
    "# Ranking results with TF-IDF\n",
    "scores_q1 = irk.tf_idf(inverted_index, query1, token_tweets)\n",
    "socres_q1 = irk.sort_scores_tf_idf(scores_q1, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
